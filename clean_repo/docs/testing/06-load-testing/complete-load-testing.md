# Load Testing - Completo

## DescripciÃ³n General

El **Load Testing Framework** de Silhouette Workflow Creation es un sistema integral de testing de carga que garantiza que la plataforma pueda manejar cargas de trabajo altas de manera eficiente y confiable. Este framework implementa testing de carga de usuarios concurrentes, testing de base de datos, testing de API, detecciÃ³n de memory leaks y testing de escalabilidad con mÃ©tricas automÃ¡ticas de performance.

## Arquitectura del Load Testing

### Componentes del Sistema de Load Testing

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   LOAD TESTING FRAMEWORK                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ CONCURRENT USER â”‚  â”‚ DATABASE LOAD   â”‚  â”‚ API LOAD     â”‚ â”‚
â”‚  â”‚ SIMULATION      â”‚  â”‚ TESTING         â”‚  â”‚ TESTING      â”‚ â”‚
â”‚  â”‚                 â”‚  â”‚                 â”‚  â”‚              â”‚ â”‚
â”‚  â”‚ â€¢ Real Users    â”‚  â”‚ â€¢ Query Load    â”‚  â”‚ â€¢ Endpoints  â”‚ â”‚
â”‚  â”‚ â€¢ Think Time    â”‚  â”‚ â€¢ Connections   â”‚  â”‚ â€¢ Rate Limit â”‚ â”‚
â”‚  â”‚ â€¢ User Behavior â”‚  â”‚ â€¢ Deadlocks     â”‚  â”‚ â€¢ Response   â”‚ â”‚
â”‚  â”‚ â€¢ Journeys      â”‚  â”‚ â€¢ Performance   â”‚  â”‚ â€¢ Throughput â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ MEMORY LEAK     â”‚  â”‚ SCALABILITY     â”‚  â”‚ STRESS       â”‚ â”‚
â”‚  â”‚ DETECTION       â”‚  â”‚ TESTING         â”‚  â”‚ TESTING      â”‚ â”‚
â”‚  â”‚                 â”‚  â”‚                 â”‚  â”‚              â”‚ â”‚
â”‚  â”‚ â€¢ Heap Analysis â”‚  â”‚ â€¢ Auto-scaling  â”‚  â”‚ â€¢ Breaking   â”‚ â”‚
â”‚  â”‚ â€¢ GC Patterns   â”‚  â”‚ â€¢ Load Balancer â”‚  â”‚ â€¢ Limits     â”‚ â”‚
â”‚  â”‚ â€¢ Memory Growth â”‚  â”‚ â€¢ Horizontal    â”‚  â”‚ â€¢ Failure    â”‚ â”‚
â”‚  â”‚ â€¢ Resource Usageâ”‚  â”‚ â€¢ Vertical      â”‚  â”‚ â€¢ Recovery   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ ENDURANCE       â”‚  â”‚ VOLUME TESTING  â”‚  â”‚ REAL-WORLD   â”‚ â”‚
â”‚  â”‚ TESTING         â”‚  â”‚                 â”‚  â”‚ SIMULATION   â”‚ â”‚
â”‚  â”‚                 â”‚  â”‚ â€¢ Large Datasetsâ”‚  â”‚              â”‚ â”‚
â”‚  â”‚ â€¢ 24/7 Testing  â”‚  â”‚ â€¢ File Size     â”‚  â”‚ â€¢ User       â”‚ â”‚
â”‚  â”‚ â€¢ Stability     â”‚  â”‚ â€¢ Data Volume   â”‚  â”‚ â€¢ Patterns   â”‚ â”‚
â”‚  â”‚ â€¢ Degradation   â”‚  â”‚ â€¢ Storage       â”‚  â”‚ â€¢ Peak Times â”‚ â”‚
â”‚  â”‚ â€¢ Resource Mgmt â”‚  â”‚ â€¢ Throughput    â”‚  â”‚ â€¢ Seasonalityâ”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 1. Concurrent User Simulation

### K6 Concurrent User Load Test

```javascript
// load-tests/concurrent-users/concurrent-load-test.js
import http from 'k6/http';
import { check, sleep } from 'k6';
import { Rate, Trend, Counter } from 'k6/metrics';

// Custom metrics
export const errorRate = new Rate('errors');
export const responseTime = new Trend('response_time');
export const userSessions = new Counter('user_sessions');
export const concurrentUsers = new Counter('concurrent_users');

// Load test configuration
export const options = {
  scenarios: {
    // Realistic user behavior simulation
    workflow_creation_load: {
      executor: 'constant-vus',
      vus: 100,
      duration: '10m',
      startTime: '0s',
    },
    workflow_execution_load: {
      executor: 'constant-vus',
      vus: 50,
      duration: '10m',
      startTime: '0s',
    },
    user_management_load: {
      executor: 'constant-vus',
      vus: 30,
      duration: '10m',
      startTime: '0s',
    },
    analytics_load: {
      executor: 'constant-vus',
      vus: 20,
      duration: '10m',
      startTime: '0s',
    },
  },
  thresholds: {
    http_req_duration: ['p(95)<1000', 'p(99)<2000'], // 95% under 1s, 99% under 2s
    http_req_failed: ['rate<0.02'], // Error rate under 2%
    errors: ['rate<0.02'],
    user_sessions: ['rate>0.95'], // 95% of user sessions should complete
  },
};

const BASE_URL = __ENV.BASE_URL || 'http://localhost:8000/api';
const TOTAL_USERS = 200;

export function setup() {
  console.log(`Starting concurrent user load test with ${TOTAL_USERS} users`);
  console.log(`Base URL: ${BASE_URL}`);

  // Create test users for concurrent testing
  const testUsers = [];
  for (let i = 0; i < TOTAL_USERS; i++) {
    const user = {
      email: `loadtest-user-${i}@silhouette-workflow.com`,
      password: 'LoadTest123!',
      name: `Load Test User ${i}`,
    };
    testUsers.push(user);
  }

  return { testUsers, baseUrl: BASE_URL };
}

export default function (data) {
  const { testUsers, baseUrl } = data;
  
  // Select a random user for this VU
  const userIndex = __VU % testUsers.length;
  const currentUser = testUsers[userIndex];
  
  // Track concurrent user
  concurrentUsers.add(1);
  
  // Authenticate user
  const authToken = authenticateUser(baseUrl, currentUser);
  
  if (!authToken) {
    errorRate.add(1);
    return;
  }
  
  userSessions.add(1);
  
  // Simulate realistic user journey
  simulateUserJourney(baseUrl, authToken, currentUser);
  
  // Simulate think time (user reading response, thinking, etc.)
  sleep(Math.random() * 3 + 2); // 2-5 seconds think time
}

function authenticateUser(baseUrl, user) {
  const authResponse = http.post(
    `${baseUrl}/auth/login`,
    JSON.stringify({
      email: user.email,
      password: user.password,
    }),
    {
      headers: { 'Content-Type': 'application/json' },
    }
  );
  
  const authCheck = check(authResponse, {
    'authentication successful': (r) => r.status === 200,
    'authentication response time < 500ms': (r) => r.timings.duration < 500,
  });
  
  errorRate.add(!authCheck);
  responseTime.add(authResponse.timings.duration);
  
  if (authCheck) {
    return authResponse.json('data.token');
  }
  
  return null;
}

function simulateUserJourney(baseUrl, authToken, user) {
  const headers = {
    'Content-Type': 'application/json',
    'Authorization': `Bearer ${authToken}`,
  };
  
  // 30% chance: Create new workflow
  if (Math.random() < 0.3) {
    createWorkflow(baseUrl, headers, user);
  }
  
  // 50% chance: View existing workflows
  if (Math.random() < 0.5) {
    listWorkflows(baseUrl, headers);
  }
  
  // 20% chance: Execute workflow
  if (Math.random() < 0.2) {
    executeWorkflow(baseUrl, headers);
  }
  
  // 25% chance: View analytics
  if (Math.random() < 0.25) {
    viewAnalytics(baseUrl, headers);
  }
  
  // 15% chance: Update user profile
  if (Math.random() < 0.15) {
    updateProfile(baseUrl, headers, user);
  }
  
  // 10% chance: Test API endpoints
  if (Math.random() < 0.1) {
    testAPIEndpoints(baseUrl, headers);
  }
}

function createWorkflow(baseUrl, headers, user) {
  const workflowData = {
    name: `${user.name} Workflow ${Date.now()} ${Math.random().toString(36).substr(2, 9)}`,
    description: `Load test workflow created at ${new Date().toISOString()}`,
    triggers: [
      {
        type: 'manual',
        config: {},
      },
    ],
    actions: [
      {
        type: 'api_call',
        config: {
          url: 'https://httpbin.org/get',
          method: 'GET',
          timeout: 10000,
        },
      },
    ],
  };
  
  const createResponse = http.post(
    `${baseUrl}/workflows`,
    JSON.stringify(workflowData),
    { headers }
  );
  
  const createCheck = check(createResponse, {
    'workflow creation status is 201': (r) => r.status === 201,
    'workflow creation response time < 1000ms': (r) => r.timings.duration < 1000,
    'workflow contains valid data': (r) => {
      const body = r.json('data');
      return body && body.id && body.name;
    },
  });
  
  errorRate.add(!createCheck);
  responseTime.add(createResponse.timings.duration);
  
  return createCheck ? createResponse.json('data.id') : null;
}

function listWorkflows(baseUrl, headers) {
  const listResponse = http.get(
    `${baseUrl}/workflows?page=1&limit=20&sort=createdAt&order=desc`,
    { headers }
  );
  
  const listCheck = check(listResponse, {
    'workflow list status is 200': (r) => r.status === 200,
    'workflow list response time < 500ms': (r) => r.timings.duration < 500,
    'workflow list contains data': (r) => {
      const body = r.json('data');
      return Array.isArray(body);
    },
  });
  
  errorRate.add(!listCheck);
  responseTime.add(listResponse.timings.duration);
}

function executeWorkflow(baseUrl, headers) {
  // First, get a list of workflows to execute
  const listResponse = http.get(`${baseUrl}/workflows?limit=10`, { headers });
  
  if (listResponse.status === 200) {
    const workflows = listResponse.json('data');
    
    if (workflows && workflows.length > 0) {
      const workflowToExecute = workflows[Math.floor(Math.random() * workflows.length)];
      
      const executeResponse = http.post(
        `${baseUrl}/workflows/${workflowToExecute.id}/execute`,
        JSON.stringify({ trigger: 'manual' }),
        { headers }
      );
      
      const executeCheck = check(executeResponse, {
        'workflow execution status is 200': (r) => r.status === 200,
        'workflow execution response time < 2000ms': (r) => r.timings.duration < 2000,
        'workflow execution started': (r) => {
          const body = r.json('data');
          return body && body.status === 'running';
        },
      });
      
      errorRate.add(!executeCheck);
      responseTime.add(executeResponse.timings.duration);
    }
  }
}

function viewAnalytics(baseUrl, headers) {
  const analyticsResponse = http.get(`${baseUrl}/analytics/dashboard`, { headers });
  
  const analyticsCheck = check(analyticsResponse, {
    'analytics status is 200': (r) => r.status === 200,
    'analytics response time < 800ms': (r) => r.timings.duration < 800,
    'analytics contains metrics': (r) => {
      const body = r.json('data');
      return body && body.summary;
    },
  });
  
  errorRate.add(!analyticsCheck);
  responseTime.add(analyticsResponse.timings.duration);
}

function updateProfile(baseUrl, headers, user) {
  const updateData = {
    name: `${user.name} (Updated)`,
    preferences: {
      theme: Math.random() > 0.5 ? 'light' : 'dark',
      notifications: Math.random() > 0.3,
    },
  };
  
  const updateResponse = http.put(
    `${baseUrl}/auth/profile`,
    JSON.stringify(updateData),
    { headers }
  );
  
  const updateCheck = check(updateResponse, {
    'profile update status is 200': (r) => r.status === 200,
    'profile update response time < 300ms': (r) => r.timings.duration < 300,
  });
  
  errorRate.add(!updateCheck);
  responseTime.add(updateResponse.timings.duration);
}

function testAPIEndpoints(baseUrl, headers) {
  // Test various API endpoints for load testing
  
  // Health check
  const healthResponse = http.get(`${baseUrl}/health`, { headers });
  check(healthResponse, {
    'health check status is 200': (r) => r.status === 200,
  });
  
  // User profile
  const profileResponse = http.get(`${baseUrl}/auth/profile`, { headers });
  check(profileResponse, {
    'profile check status is 200': (r) => r.status === 200,
  });
  
  // System metrics (if available)
  const metricsResponse = http.get(`${baseUrl}/metrics`, { headers });
  check(metricsResponse, {
    'metrics check status is 200': (r) => r.status === 200,
  });
}
```

### JMeter Concurrent User Test Plan

```xml
<?xml version="1.0" encoding="UTF-8"?>
<jmeterTestPlan version="1.2" properties="5.0" jmeter="5.6.2">
  <hashTree>
    <!-- Test Plan for Concurrent User Load Testing -->
    <TestPlan guiclass="TestPlanGui" testclass="TestPlan" testname="Concurrent User Load Test" enabled="true">
      <stringProp name="TestPlan.user_define_classpath"></stringProp>
      <boolProp name="TestPlan.functional_mode">false</boolProp>
      <boolProp name="TestPlan.comments">Concurrent user load testing for Silhouette Workflow platform</boolProp>
      <stringProp name="TestPlan.user_define_classpath"></stringProp>
      <elementProp name="TestPlan.user_defined_variables" elementType="Arguments" guiclass="ArgumentsPanel" testclass="Arguments" testname="User Defined Variables" enabled="true">
        <collectionProp name="Arguments.arguments">
          <elementProp name="baseUrl" elementType="Argument">
            <stringProp name="Argument.name">baseUrl</stringProp>
            <stringProp name="Argument.value">http://localhost:8000/api</stringProp>
            <stringProp name="Argument.metadata">=</stringProp>
          </elementProp>
          <elementProp name="concurrentUsers" elementType="Argument">
            <stringProp name="Argument.name">concurrentUsers</stringProp>
            <stringProp name="Argument.value">500</stringProp>
            <stringProp name="Argument.metadata">=</stringProp>
          </elementProp>
          <elementProp name="rampUpTime" elementType="Argument">
            <stringProp name="Argument.name">rampUpTime</stringProp>
            <stringProp name="Argument.value">300</stringProp>
            <stringProp name="Argument.metadata">=</stringProp>
          </elementProp>
          <elementProp name="testDuration" elementType="Argument">
            <stringProp name="Argument.name">testDuration</stringProp>
            <stringProp name="Argument.value">1800</stringProp>
            <stringProp name="Argument.metadata">=</stringProp>
          </elementProp>
        </collectionProp>
      </elementProp>
    </TestPlan>
    
    <hashTree>
      <!-- Main Thread Group for Concurrent Users -->
      <ThreadGroup guiclass="ThreadGroupGui" testclass="ThreadGroup" testname="Concurrent Users Thread Group" enabled="true">
        <stringProp name="ThreadGroup.on_sample_error">continue</stringProp>
        <elementProp name="ThreadGroup.main_controller" elementType="LoopController" guiclass="LoopControllerGui" testclass="LoopController" testname="Loop Controller" enabled="true">
          <boolProp name="LoopController.continue_forever">false</boolProp>
          <stringProp name="LoopController.loops">-1</stringProp>
        </elementProp>
        <stringProp name="ThreadGroup.num_threads">${concurrentUsers}</stringProp>
        <stringProp name="ThreadGroup.ramp_time">${rampUpTime}</stringProp>
        <stringProp name="ThreadGroup.duration">${testDuration}</stringProp>
        <stringProp name="ThreadGroup.delay">0</stringProp>
        <boolProp name="ThreadGroup.scheduler">true</boolProp>
      </ThreadGroup>
      
      <hashTree>
        <!-- User Data Set Config for test users -->
        <ConfigTestElement guiclass="CsvDataSetGui" testclass="ConfigTestElement" testname="CSV Data Set Config - Users" enabled="true">
          <stringProp name="delimiter">,</stringProp>
          <stringProp name="fileEncoding">UTF-8</stringProp>
          <stringProp name="filename">${__P(workspace)}/load-tests/data/concurrent-users.csv</stringProp>
          <boolProp name="ignoreFirstLine">true</boolProp>
          <boolProp name="quotedData">false</boolProp>
          <boolProp name="recycle">true</boolProp>
          <stringProp name="shareMode">shareMode.all</stringProp>
          <stringProp name="variableNames">email,password,name,workflowCount,executionCount</stringProp>
        </ConfigTestElement>
        
        <!-- HTTP Header Manager -->
        <HeaderManager guiclass="HeaderPanel" testclass="HeaderManager" testname="HTTP Header Manager" enabled="true">
          <collectionProp name="HeaderManager.headers">
            <elementProp name="" elementType="Header">
              <stringProp name="Header.name">Content-Type</stringProp>
              <stringProp name="Header.value">application/json</stringProp>
            </elementProp>
            <elementProp name="" elementType="Header">
              <stringProp name="Header.name">Accept</stringProp>
              <stringProp name="Header.value">application/json</stringProp>
            </elementProp>
            <elementProp name="" elementType="Header">
              <stringProp name="Header.name">User-Agent</stringProp>
              <stringProp name="Header.value">Silhouette-LoadTest/1.0</stringProp>
            </elementProp>
          </collectionProp>
        </HeaderManager>
        
        <!-- Random Timer for Think Time -->
        <RandomTimer guiclass="RandomTimerGui" testclass="RandomTimer" testname="User Think Time" enabled="true">
          <stringProp name="DelayOffset">2000</stringProp>
          <stringProp name="MaximumRandomRange">3000</stringProp>
        </RandomTimer>
        
        <!-- Authentication Controller -->
        <hashTree>
          <IfController guiclass="IfControllerPanel" testclass="IfController" testname="Authentication Controller" enabled="true">
            <stringProp name="IfController.condition">${__jexl3("${authToken}" == "")}</stringProp>
          </IfController>
          
          <hashTree>
            <HTTPSamplerProxy guiclass="HttpTestSampleGui" testclass="HTTPSamplerProxy" testname="User Authentication" enabled="true">
              <elementProp name="HTTPsampler.Arguments" elementType="Arguments" guiclass="ArgumentsPanel" testclass="Arguments" testname="User Defined Variables" enabled="true">
                <collectionProp name="Arguments.arguments">
                  <elementProp name="email" elementType="Argument">
                    <stringProp name="Argument.name">email</stringProp>
                    <stringProp name="Argument.value">${email}</stringProp>
                    <stringProp name="Argument.metadata">=</stringProp>
                  </elementProp>
                  <elementProp name="password" elementType="Argument">
                    <stringProp name="Argument.name">password</stringProp>
                    <stringProp name="Argument.value">${password}</stringProp>
                    <stringProp name="Argument.metadata">=</stringProp>
                  </elementProp>
                </collectionProp>
              </elementProp>
              <stringProp name="HTTPSampler.domain">${baseUrl}</stringProp>
              <stringProp name="HTTPSampler.port"></stringProp>
              <stringProp name="HTTPSampler.path">/auth/login</stringProp>
              <stringProp name="HTTPSampler.method">POST</stringProp>
              <boolProp name="HTTPSampler.follow_redirects">true</boolProp>
              <boolProp name="HTTPSampler.auto_redirects">false</boolProp>
              <boolProp name="HTTPSampler.use_keepalive">true</boolProp>
              <boolProp name="HTTPSampler.DO_MULTIPART_POST">false</boolProp>
              <stringProp name="HTTPSampler.embedded_url_re"></stringProp>
              <stringProp name="HTTPSampler.connect_timeout">30000</stringProp>
              <stringProp name="HTTPSampler.response_timeout">30000</stringProp>
            </HTTPSamplerProxy>
            
            <hashTree>
              <!-- JSON Extractor for auth token -->
              <JSONExtractor guiclass="JSONPostProcessorGui" testclass="JSONExtractor" testname="JSON Extractor - Auth Token" enabled="true">
                <stringProp name="JSONExtractor.referenceNames">authToken</stringProp>
                <stringProp name="JSONExtractor.jsonPathExpressions">$.data.token</stringProp>
                <stringProp name="JSONExtractor.match_numbers">1</stringProp>
                <stringProp name="JSONExtractor.defaultValues">NOT_FOUND</stringProp>
              </JSONExtractor>
              
              <!-- Response Assertion for authentication -->
              <ResponseAssertion guiclass="AssertionGui" testclass="ResponseAssertion" testname="Response Assertion - Authentication Success" enabled="true">
                <collectionProp name="Asserion.test_strings">
                  <stringProp name="49586">200</stringProp>
                </collectionProp>
                <stringProp name="Assertion.custom_message">Authentication failed for user ${email}</stringProp>
                <stringProp name="Assertion.test_field">Assertion.response_code</stringProp>
                <boolProp name="Assertion.assume_success">false</boolProp>
                <intProp name="Assertion.test_type">2</intProp>
              </ResponseAssertion>
            </hashTree>
          </hashTree>
        </hashTree>
        
        <!-- Add Auth Header -->
        <hashTree>
          <HeaderManager guiclass="HeaderPanel" testclass="HeaderManager" testname="Authorization Header" enabled="true">
            <collectionProp name="HeaderManager.headers">
              <elementProp name="" elementType="Header">
                <stringProp name="Header.name">Authorization</stringProp>
                <stringProp name="Header.value">Bearer ${authToken}</stringProp>
              </elementProp>
            </collectionProp>
          </HeaderManager>
        </hashTree>
        
        <!-- Workflow Operations Controller -->
        <hashTree>
          <RandomController guiclass="RandomControllerGui" testclass="RandomController" testname="Random Workflow Operations" enabled="true">
            <boolProp name="RandomController.resetSeed">false</boolProp>
            <stringProp name="RandomController.weight">1</stringProp>
          </RandomController>
          
          <hashTree>
            <!-- Create Workflow (30% probability) -->
            <IfController guiclass="IfControllerPanel" testclass="IfController" testname="Create Workflow Controller" enabled="true">
              <stringProp name="IfController.condition">${__jexl3("${workflowCount}" == "" || ${workflowCount} < 5)}</stringProp>
            </IfController>
            
            <hashTree>
              <HTTPSamplerProxy guiclass="HttpTestSampleGui" testclass="HTTPSamplerProxy" testname="Create Workflow" enabled="true">
                <elementProp name="HTTPsampler.Arguments" elementType="Arguments" guiclass="ArgumentsPanel" testclass="Arguments" testname="User Defined Variables" enabled="true">
                  <collectionProp name="Arguments.arguments">
                    <elementProp name="workflowName" elementType="Argument">
                      <stringProp name="Argument.name">workflowName</stringProp>
                      <stringProp name="Argument.value">Load Test Workflow ${__RandomString(8,abcdefghijklmnopqrstuvwxyz)} ${__time()}</stringProp>
                      <stringProp name="Argument.metadata">=</stringProp>
                    </elementProp>
                    <elementProp name="description" elementType="Argument">
                      <stringProp name="Argument.name">description</stringProp>
                      <stringProp name="Argument.value">Concurrent load test workflow created at ${__time()}</stringProp>
                      <stringProp name="Argument.metadata">=</stringProp>
                    </elementProp>
                  </collectionProp>
                </elementProp>
                <stringProp name="HTTPSampler.domain">${baseUrl}</stringProp>
                <stringProp name="HTTPSampler.port"></stringProp>
                <stringProp name="HTTPSampler.path">/workflows</stringProp>
                <stringProp name="HTTPSampler.method">POST</stringProp>
                <boolProp name="HTTPSampler.follow_redirects">true</boolProp>
                <boolProp name="HTTPSampler.auto_redirects">false</boolProp>
                <boolProp name="HTTPSampler.use_keepalive">true</boolProp>
                <boolProp name="HTTPSampler.DO_MULTIPART_POST">false</boolProp>
                <stringProp name="HTTPSampler.embedded_url_re"></stringProp>
                <stringProp name="HTTPSampler.connect_timeout">30000</stringProp>
                <stringProp name="HTTPSampler.response_timeout">30000</stringProp>
                <boolProp name="HTTPSampler.postBodyRaw">true</boolProp>
                <elementProp name="HTTPsampler.Arguments">
                  <stringProp name="Argument.value">{"name": "${workflowName}", "description": "${description}", "triggers": [{"type": "manual"}], "actions": [{"type": "api_call", "config": {"url": "https://httpbin.org/get", "method": "GET"}}]}</stringProp>
                  <stringProp name="Argument.metadata">=</stringProp>
                </elementProp>
              </HTTPSamplerProxy>
              
              <hashTree>
                <!-- JSON Extractor for workflow ID -->
                <JSONExtractor guiclass="JSONPostProcessorGui" testclass="JSONExtractor" testname="JSON Extractor - Workflow ID" enabled="true">
                  <stringProp name="JSONExtractor.referenceNames">workflowId</stringProp>
                  <stringProp name="JSONExtractor.jsonPathExpressions">$.data.id</stringProp>
                  <stringProp name="JSONExtractor.match_numbers">1</stringProp>
                  <stringProp name="JSONExtractor.defaultValues">NOT_FOUND</stringProp>
                </JSONExtractor>
                
                <!-- Response Assertions -->
                <ResponseAssertion guiclass="AssertionGui" testclass="ResponseAssertion" testname="Response Assertion - Create Success" enabled="true">
                  <collectionProp name="Asserion.test_strings">
                    <stringProp name="49586">201</stringProp>
                  </collectionProp>
                  <stringProp name="Assertion.custom_message">Workflow creation failed</stringProp>
                  <stringProp name="Assertion.test_field">Assertion.response_code</stringProp>
                  <boolProp name="Assertion.assume_success">false</boolProp>
                  <intProp name="Assertion.test_type">8</intProp>
                </ResponseAssertion>
              </hashTree>
            </hashTree>
            
            <!-- List Workflows (50% probability) -->
            <IfController guiclass="IfControllerPanel" testclass="IfController" testname="List Workflows Controller" enabled="true">
              <stringProp name="IfController.condition">${__Random(1,100)} &lt;= 50</stringProp>
            </IfController>
            
            <hashTree>
              <HTTPSamplerProxy guiclass="HttpTestSampleGui" testclass="HTTPSamplerProxy" testname="List Workflows" enabled="true">
                <stringProp name="HTTPSampler.domain">${baseUrl}</stringProp>
                <stringProp name="HTTPSampler.port"></stringProp>
                <stringProp name="HTTPSampler.path">/workflows?page=1&limit=20&sort=createdAt&order=desc</stringProp>
                <stringProp name="HTTPSampler.method">GET</stringProp>
                <boolProp name="HTTPSampler.follow_redirects">true</boolProp>
                <boolProp name="HTTPSampler.auto_redirects">false</boolProp>
                <boolProp name="HTTPSampler.use_keepalive">true</boolProp>
                <boolProp name="HTTPSampler.DO_MULTIPART_POST">false</boolProp>
                <stringProp name="HTTPSampler.embedded_url_re"></stringProp>
                <stringProp name="HTTPSampler.connect_timeout">30000</stringProp>
                <stringProp name="HTTPSampler.response_timeout">30000</stringProp>
              </HTTPSamplerProxy>
              
              <hashTree>
                <ResponseAssertion guiclass="AssertionGui" testclass="ResponseAssertion" testname="Response Assertion - List Success" enabled="true">
                  <collectionProp name="Asserion.test_strings">
                    <stringProp name="49586">200</stringProp>
                  </collectionProp>
                  <stringProp name="Assertion.custom_message">Workflow listing failed</stringProp>
                  <stringProp name="Assertion.test_field">Assertion.response_code</stringProp>
                  <boolProp name="Assertion.assume_success">false</boolProp>
                  <intProp name="Assertion.test_type">8</intProp>
                </ResponseAssertion>
              </hashTree>
            </hashTree>
            
            <!-- Execute Workflow (20% probability) -->
            <IfController guiclass="IfControllerPanel" testclass="IfController" testname="Execute Workflow Controller" enabled="true">
              <stringProp name="IfController.condition">${__Random(1,100)} &lt;= 20</stringProp>
            </IfController>
            
            <hashTree>
              <HTTPSamplerProxy guiclass="HttpTestSampleGui" testclass="HTTPSamplerProxy" testname="Execute Workflow" enabled="true">
                <elementProp name="HTTPsampler.Arguments" elementType="Arguments" guiclass="ArgumentsPanel" testclass="Arguments" testname="User Defined Variables" enabled="true">
                  <collectionProp name="Arguments.arguments">
                    <elementProp name="trigger" elementType="Argument">
                      <stringProp name="Argument.name">trigger</stringProp>
                      <stringProp name="Argument.value">manual</stringProp>
                      <stringProp name="Argument.metadata">=</stringProp>
                    </elementProp>
                  </collectionProp>
                </elementProp>
                <stringProp name="HTTPSampler.domain">${baseUrl}</stringProp>
                <stringProp name="HTTPSampler.port"></stringProp>
                <stringProp name="HTTPSampler.path">/workflows/${workflowId}/execute</stringProp>
                <stringProp name="HTTPSampler.method">POST</stringProp>
                <boolProp name="HTTPSampler.follow_redirects">true</boolProp>
                <boolProp name="HTTPSampler.auto_redirects">false</boolProp>
                <boolProp name="HTTPSampler.use_keepalive">true</boolProp>
                <boolProp name="HTTPSampler.DO_MULTIPART_POST">false</boolProp>
                <stringProp name="HTTPSampler.embedded_url_re"></stringProp>
                <stringProp name="HTTPSampler.connect_timeout">30000</stringProp>
                <stringProp name="HTTPSampler.response_timeout">30000</stringProp>
                <boolProp name="HTTPSampler.postBodyRaw">true</boolProp>
                <elementProp name="HTTPsampler.Arguments">
                  <stringProp name="Argument.value">{"trigger": "${trigger}"}</stringProp>
                  <stringProp name="Argument.metadata">=</stringProp>
                </elementProp>
              </HTTPSamplerProxy>
              
              <hashTree>
                <ResponseAssertion guiclass="AssertionGui" testclass="ResponseAssertion" testname="Response Assertion - Execute Success" enabled="true">
                  <collectionProp name="Asserion.test_strings">
                    <stringProp name="49586">200</stringProp>
                  </collectionProp>
                  <stringProp name="Assertion.custom_message">Workflow execution failed</stringProp>
                  <stringProp name="Assertion.test_field">Assertion.response_code</stringProp>
                  <boolProp name="Assertion.assume_success">false</boolProp>
                  <intProp name="Assertion.test_type">8</intProp>
                </ResponseAssertion>
              </hashTree>
            </hashTree>
          </hashTree>
        </hashTree>
        
        <!-- User Profile Operations -->
        <hashTree>
          <IfController guiclass="IfControllerPanel" testclass="IfController" testname="Profile Operations Controller" enabled="true">
            <stringProp name="IfController.condition">${__Random(1,100)} &lt;= 15</stringProp>
          </IfController>
          
          <hashTree>
            <HTTPSamplerProxy guiclass="HttpTestSampleGui" testclass="HTTPSamplerProxy" testname="Get User Profile" enabled="true">
              <stringProp name="HTTPSampler.domain">${baseUrl}</stringProp>
              <stringProp name="HTTPSampler.port"></stringProp>
              <stringProp name="HTTPSampler.path">/auth/profile</stringProp>
              <stringProp name="HTTPSampler.method">GET</stringProp>
              <boolProp name="HTTPSampler.follow_redirects">true</boolProp>
              <boolProp name="HTTPSampler.auto_redirects">false</boolProp>
              <boolProp name="HTTPSampler.use_keepalive">true</boolProp>
              <boolProp name="HTTPSampler.DO_MULTIPART_POST">false</boolProp>
              <stringProp name="HTTPSampler.embedded_url_re"></stringProp>
              <stringProp name="HTTPSampler.connect_timeout">30000</stringProp>
              <stringProp name="HTTPSampler.response_timeout">30000</stringProp>
            </HTTPSamplerProxy>
          </hashTree>
        </hashTree>
        
        <!-- Analytics Operations -->
        <hashTree>
          <IfController guiclass="IfControllerPanel" testclass="IfController" testname="Analytics Operations Controller" enabled="true">
            <stringProp name="IfController.condition">${__Random(1,100)} &lt;= 25</stringProp>
          </IfController>
          
          <hashTree>
            <HTTPSamplerProxy guiclass="HttpTestSampleGui" testclass="HTTPSamplerProxy" testname="Get Analytics Dashboard" enabled="true">
              <stringProp name="HTTPSampler.domain">${baseUrl}</stringProp>
              <stringProp name="HTTPSampler.port"></stringProp>
              <stringProp name="HTTPSampler.path">/analytics/dashboard</stringProp>
              <stringProp name="HTTPSampler.method">GET</stringProp>
              <boolProp name="HTTPSampler.follow_redirects">true</boolProp>
              <boolProp name="HTTPSampler.auto_redirects">false</boolProp>
              <boolProp name="HTTPSampler.use_keepalive">true</boolProp>
              <boolProp name="HTTPSampler.DO_MULTIPART_POST">false</boolProp>
              <stringProp name="HTTPSampler.embedded_url_re"></stringProp>
              <stringProp name="HTTPSampler.connect_timeout">30000</stringProp>
              <stringProp name="HTTPSampler.response_timeout">30000</stringProp>
            </HTTPSamplerProxy>
          </hashTree>
        </hashTree>
        
        <!-- Logout Controller -->
        <hashTree>
          <IfController guiclass="IfControllerPanel" testclass="IfController" testname="Logout Controller" enabled="true">
            <stringProp name="IfController.condition">${__Random(1,100)} &lt;= 10</stringProp>
          </IfController>
          
          <hashTree>
            <HTTPSamplerProxy guiclass="HttpTestSampleGui" testclass="HTTPSamplerProxy" testname="User Logout" enabled="true">
              <stringProp name="HTTPSampler.domain">${baseUrl}</stringProp>
              <stringProp name="HTTPSampler.port"></stringProp>
              <stringProp name="HTTPSampler.path">/auth/logout</stringProp>
              <stringProp name="HTTPSampler.method">POST</stringProp>
              <boolProp name="HTTPSampler.follow_redirects">true</boolProp>
              <boolProp name="HTTPSampler.auto_redirects">false</boolProp>
              <boolProp name="HTTPSampler.use_keepalive">true</boolProp>
              <boolProp name="HTTPSampler.DO_MULTIPART_POST">false</boolProp>
              <stringProp name="HTTPSampler.embedded_url_re"></stringProp>
              <stringProp name="HTTPSampler.connect_timeout">30000</stringProp>
              <stringProp name="HTTPSampler.response_timeout">30000</stringProp>
            </HTTPSamplerProxy>
          </hashTree>
        </hashTree>
      </hashTree>
      
      <!-- Performance Listeners -->
      <hashTree>
        <ResultCollector guiclass="SummaryReport" testclass="ResultCollector" testname="Summary Report" enabled="true">
          <boolProp name="ResultCollector.error_logging">false</boolProp>
          <objProp>
            <name>saveConfig</name>
            <value class="SampleSaveConfiguration">
              <time>true</time>
              <latency>true</latency>
              <timestamp>true</timestamp>
              <success>true</success>
              <label>true</label>
              <code>true</code>
              <message>true</message>
              <threadName>true</threadName>
              <dataType>true</dataType>
              <encoding>false</encoding>
              <assertions>true</assertions>
              <subresults>true</subresults>
              <responseData>false</responseData>
              <samplerData>false</samplerData>
              <xml>false</xml>
              <fieldNames>true</fieldNames>
              <responseHeaders>false</responseHeaders>
              <requestHeaders>false</requestHeaders>
              <responseDataOnError>false</responseDataOnError>
              <saveAssertionResultsFailureMessage>true</saveAssertionResultsFailureMessage>
              <assertionsResultsToSave>0</assertionsResultsToSave>
              <bytes>true</bytes>
              <sentBytes>true</sentBytes>
              <url>true</url>
              <threadCounts>true</threadCounts>
              <idleTime>true</idleTime>
              <connectTime>true</connectTime>
            </value>
          </objProp>
          <stringProp name="filename">${__P(workspace)}/load-tests/results/concurrent-load-test.jtl</stringProp>
        </ResultCollector>
        
        <ResultCollector guiclass="GraphVisualizer" testclass="ResultCollector" testname="Response Time Graph" enabled="true">
          <boolProp name="ResultCollector.error_logging">false</boolProp>
          <objProp>
            <name>saveConfig</name>
            <value class="SampleSaveConfiguration">
              <time>true</time>
              <latency>true</latency>
              <timestamp>true</timestamp>
              <success>true</success>
              <label>true</label>
              <code>true</code>
              <message>true</message>
              <threadName>true</threadName>
              <dataType>true</dataType>
              <encoding>false</encoding>
              <assertions>true</assertions>
              <subresults>true</subresults>
              <responseData>false</responseData>
              <samplerData>false</samplerData>
              <xml>false</xml>
              <fieldNames>true</fieldNames>
              <responseHeaders>false</responseHeaders>
              <requestHeaders>false</requestHeaders>
              <responseDataOnError>false</responseDataOnError>
              <saveAssertionResultsFailureMessage>true</saveAssertionResultsFailureMessage>
              <assertionsResultsToSave>0</assertionsResultsToSave>
              <bytes>true</bytes>
              <sentBytes>true</sentBytes>
              <url>true</url>
              <threadCounts>true</threadCounts>
              <idleTime>true</idleTime>
              <connectTime>true</connectTime>
            </value>
          </objProp>
          <stringProp name="filename">${__P(workspace)}/load-tests/results/response-time-graph.jtl</stringProp>
        </ResultCollector>
      </hashTree>
    </hashTree>
  </hashTree>
</jmeterTestPlan>
```

## 2. Database Load Testing

### Database Performance Testing Script

```python
#!/usr/bin/env python3
# load-tests/database/database-load-test.py
import asyncio
import asyncpg
import asyncpg.pool
import json
import time
import random
from datetime import datetime
from typing import List, Dict, Any
import statistics

class DatabaseLoadTester:
    def __init__(self, db_config: Dict[str, Any]):
        self.db_config = db_config
        self.pool = None
        self.results = {
            'connection_tests': [],
            'query_tests': [],
            'concurrent_tests': [],
            'throughput_tests': [],
            'error_tests': []
        }
    
    async def initialize_pool(self):
        """Initialize database connection pool"""
        try:
            self.pool = await asyncpg.create_pool(
                host=self.db_config['host'],
                port=self.db_config['port'],
                user=self.db_config['user'],
                password=self.db_config['password'],
                database=self.db_config['database'],
                min_size=10,
                max_size=50,
                command_timeout=60
            )
            print("âœ… Database connection pool initialized")
        except Exception as e:
            print(f"âŒ Failed to initialize database pool: {e}")
            raise
    
    async def test_connection_performance(self):
        """Test database connection performance"""
        print("ğŸ” Testing database connection performance...")
        
        test_results = []
        
        for i in range(100):
            start_time = time.time()
            try:
                async with self.pool.acquire() as conn:
                    await conn.fetchval('SELECT 1')
                end_time = time.time()
                duration = (end_time - start_time) * 1000  # Convert to milliseconds
                
                test_results.append({
                    'test_id': i,
                    'duration_ms': duration,
                    'success': True,
                    'timestamp': datetime.now().isoformat()
                })
                
            except Exception as e:
                test_results.append({
                    'test_id': i,
                    'duration_ms': None,
                    'success': False,
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                })
        
        self.results['connection_tests'] = test_results
        
        successful_tests = [t for t in test_results if t['success']]
        if successful_tests:
            avg_duration = statistics.mean([t['duration_ms'] for t in successful_tests])
            p95_duration = statistics.quantiles([t['duration_ms'] for t in successful_tests], n=20)[18]  # 95th percentile
            
            print(f"âœ… Connection tests completed:")
            print(f"   Success rate: {len(successful_tests)}/{len(test_results)}")
            print(f"   Average duration: {avg_duration:.2f}ms")
            print(f"   95th percentile: {p95_duration:.2f}ms")
    
    async def test_query_performance(self):
        """Test various query performance patterns"""
        print("ğŸ” Testing query performance patterns...")
        
        # Test different query types
        query_tests = [
            {
                'name': 'Simple SELECT',
                'query': 'SELECT COUNT(*) FROM users',
                'expected_rows': 1
            },
            {
                'name': 'Complex JOIN',
                'query': '''
                    SELECT w.id, w.name, u.email, COUNT(e.id) as execution_count
                    FROM workflows w
                    JOIN users u ON w.user_id = u.id
                    LEFT JOIN workflow_executions e ON w.id = e.workflow_id
                    GROUP BY w.id, w.name, u.email
                    LIMIT 100
                ''',
                'expected_rows': 100
            },
            {
                'name': 'Filtered SELECT',
                'query': 'SELECT * FROM workflows WHERE status = $1 AND created_at > $2',
                'params': ('active', '2024-01-01'),
                'expected_rows': None
            },
            {
                'name': 'Aggregation',
                'query': '''
                    SELECT 
                        DATE_TRUNC('day', created_at) as date,
                        COUNT(*) as workflow_count,
                        AVG(EXTRACT(EPOCH FROM (updated_at - created_at))) as avg_duration
                    FROM workflows 
                    WHERE created_at > NOW() - INTERVAL '30 days'
                    GROUP BY DATE_TRUNC('day', created_at)
                    ORDER BY date
                ''',
                'expected_rows': 30
            }
        ]
        
        for test in query_tests:
            test_results = []
            iterations = 50
            
            for i in range(iterations):
                start_time = time.time()
                try:
                    async with self.pool.acquire() as conn:
                        if 'params' in test:
                            result = await conn.fetch(test['query'], *test['params'])
                        else:
                            result = await conn.fetch(test['query'])
                    
                    end_time = time.time()
                    duration = (end_time - start_time) * 1000
                    row_count = len(result)
                    
                    test_results.append({
                        'test_name': test['name'],
                        'iteration': i,
                        'duration_ms': duration,
                        'row_count': row_count,
                        'success': True,
                        'timestamp': datetime.now().isoformat()
                    })
                    
                except Exception as e:
                    test_results.append({
                        'test_name': test['name'],
                        'iteration': i,
                        'duration_ms': None,
                        'row_count': 0,
                        'success': False,
                        'error': str(e),
                        'timestamp': datetime.now().isoformat()
                    })
            
            self.results['query_tests'].extend(test_results)
            
            successful_tests = [t for t in test_results if t['success']]
            if successful_tests:
                avg_duration = statistics.mean([t['duration_ms'] for t in successful_tests])
                total_rows = sum([t['row_count'] for t in successful_tests])
                
                print(f"âœ… {test['name']} completed:")
                print(f"   Average duration: {avg_duration:.2f}ms")
                print(f"   Total rows processed: {total_rows}")
                print(f"   Success rate: {len(successful_tests)}/{len(test_results)}")
    
    async def test_concurrent_queries(self):
        """Test concurrent query execution"""
        print("ğŸ” Testing concurrent query performance...")
        
        async def execute_query(query_id: int, query: str):
            start_time = time.time()
            try:
                async with self.pool.acquire() as conn:
                    result = await conn.fetch(query)
                end_time = time.time()
                duration = (end_time - start_time) * 1000
                
                return {
                    'query_id': query_id,
                    'duration_ms': duration,
                    'row_count': len(result),
                    'success': True,
                    'timestamp': datetime.now().isoformat()
                }
            except Exception as e:
                end_time = time.time()
                duration = (end_time - start_time) * 1000
                
                return {
                    'query_id': query_id,
                    'duration_ms': duration,
                    'row_count': 0,
                    'success': False,
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                }
        
        # Test different levels of concurrency
        concurrency_levels = [10, 25, 50, 100]
        
        for concurrency in concurrency_levels:
            print(f"   Testing {concurrency} concurrent queries...")
            
            queries = [
                'SELECT COUNT(*) FROM users',
                'SELECT * FROM workflows LIMIT 100',
                'SELECT * FROM workflow_executions WHERE created_at > NOW() - INTERVAL \'1 hour\' LIMIT 50',
                'SELECT u.email, COUNT(w.id) as workflow_count FROM users u LEFT JOIN workflows w ON u.id = w.user_id GROUP BY u.id, u.email',
                'SELECT * FROM analytics_events WHERE event_type = $1 LIMIT 20'
            ] * (concurrency // len(queries) + 1)
            queries = queries[:concurrency]
            
            # Execute queries concurrently
            tasks = [execute_query(i, query) for i, query in enumerate(queries)]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Filter out exceptions
            valid_results = [r for r in results if isinstance(r, dict)]
            
            self.results['concurrent_tests'].extend(valid_results)
            
            if valid_results:
                successful_results = [r for r in valid_results if r['success']]
                avg_duration = statistics.mean([r['duration_ms'] for r in successful_results])
                total_rows = sum([r['row_count'] for r in successful_results])
                
                print(f"     âœ… {concurrency} concurrent queries completed:")
                print(f"        Average duration: {avg_duration:.2f}ms")
                print(f"        Total rows: {total_rows}")
                print(f"        Success rate: {len(successful_results)}/{len(valid_results)}")
                
                # Check for any timeout or deadlocks
                timeouts = [r for r in valid_results if r['duration_ms'] > 5000]
                if timeouts:
                    print(f"        âš ï¸ {len(timeouts)} queries took longer than 5 seconds")
    
    async def test_throughput(self):
        """Test database throughput under sustained load"""
        print("ğŸ” Testing database throughput...")
        
        # Simulate continuous write operations
        async def write_operations():
            for i in range(100):
                try:
                    async with self.pool.acquire() as conn:
                        # Insert test data
                        await conn.execute('''
                            INSERT INTO test_load (test_id, data, created_at) 
                            VALUES ($1, $2, NOW())
                        ''', i, f'Load test data {i}')
                    
                    # Small delay to simulate realistic load
                    await asyncio.sleep(0.01)
                    
                except Exception as e:
                    print(f"Write operation {i} failed: {e}")
        
        # Simulate continuous read operations
        async def read_operations():
            for i in range(200):
                try:
                    async with self.pool.acquire() as conn:
                        result = await conn.fetch('SELECT * FROM test_load ORDER BY created_at DESC LIMIT 10')
                    
                    await asyncio.sleep(0.005)
                    
                except Exception as e:
                    print(f"Read operation {i} failed: {e}")
        
        start_time = time.time()
        
        # Run write and read operations concurrently
        write_task = asyncio.create_task(write_operations())
        read_task = asyncio.create_task(read_operations())
        
        await asyncio.gather(write_task, read_task)
        
        end_time = time.time()
        total_duration = end_time - start_time
        
        # Get final counts
        async with self.pool.acquire() as conn:
            final_count = await conn.fetchval('SELECT COUNT(*) FROM test_load')
        
        throughput = final_count / total_duration
        
        self.results['throughput_tests'].append({
            'test_type': 'sustained_load',
            'duration_seconds': total_duration,
            'total_operations': final_count,
            'throughput_ops_per_second': throughput,
            'timestamp': datetime.now().isoformat()
        })
        
        print(f"âœ… Throughput test completed:")
        print(f"   Duration: {total_duration:.2f} seconds")
        print(f"   Total operations: {final_count}")
        print(f"   Throughput: {throughput:.2f} ops/second")
    
    async def test_error_handling(self):
        """Test database behavior under error conditions"""
        print("ğŸ” Testing database error handling...")
        
        error_scenarios = [
            {
                'name': 'Connection Limit Test',
                'action': 'test_connection_limit'
            },
            {
                'name': 'Long Query Test',
                'action': 'test_long_query'
            },
            {
                'name': 'Deadlock Simulation',
                'action': 'test_deadlock'
            }
        ]
        
        for scenario in error_scenarios:
            print(f"   Testing {scenario['name']}...")
            
            if scenario['action'] == 'test_connection_limit':
                # Try to create more connections than allowed
                tasks = []
                for i in range(100):  # More than max_size
                    tasks.append(self.pool.acquire())
                
                # Wait for some to succeed/fail
                results = await asyncio.gather(*tasks[:20], return_exceptions=True)
                
                successful = [r for r in results if not isinstance(r, Exception)]
                failed = [r for r in results if isinstance(r, Exception)]
                
                self.results['error_tests'].append({
                    'scenario': scenario['name'],
                    'successful_connections': len(successful),
                    'failed_connections': len(failed),
                    'timestamp': datetime.now().isoformat()
                })
                
                # Release successful connections
                for conn in successful:
                    await conn.close()
                
            elif scenario['action'] == 'test_long_query':
                # Test long-running query
                start_time = time.time()
                try:
                    async with self.pool.acquire() as conn:
                        result = await conn.fetch('''
                            SELECT COUNT(*) 
                            FROM generate_series(1, 1000000) 
                            WHERE random() < 0.001
                        ''')
                    end_time = time.time()
                    duration = end_time - start_time
                    
                    self.results['error_tests'].append({
                        'scenario': scenario['name'],
                        'duration_seconds': duration,
                        'success': True,
                        'timestamp': datetime.now().isoformat()
                    })
                    
                except Exception as e:
                    self.results['error_tests'].append({
                        'scenario': scenario['name'],
                        'success': False,
                        'error': str(e),
                        'timestamp': datetime.now().isoformat()
                    })
    
    async def cleanup(self):
        """Clean up test data and close connections"""
        print("ğŸ§¹ Cleaning up test data...")
        
        try:
            async with self.pool.acquire() as conn:
                # Clean up test data
                await conn.execute('DELETE FROM test_load WHERE data LIKE \'Load test data%\'')
            
            # Close pool
            await self.pool.close()
            print("âœ… Cleanup completed")
            
        except Exception as e:
            print(f"âš ï¸ Cleanup error: {e}")
    
    async def generate_report(self, output_file: str):
        """Generate load testing report"""
        print("ğŸ“ Generating load testing report...")
        
        report = {
            'test_info': {
                'timestamp': datetime.now().isoformat(),
                'database_config': self.db_config,
                'test_duration': 'comprehensive'
            },
            'summary': {
                'connection_tests': len(self.results['connection_tests']),
                'query_tests': len(self.results['query_tests']),
                'concurrent_tests': len(self.results['concurrent_tests']),
                'throughput_tests': len(self.results['throughput_tests']),
                'error_tests': len(self.results['error_tests'])
            },
            'detailed_results': self.results,
            'metrics': self.calculate_metrics()
        }
        
        with open(output_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"âœ… Report saved to: {output_file}")
    
    def calculate_metrics(self):
        """Calculate performance metrics from test results"""
        metrics = {}
        
        # Connection metrics
        connection_tests = [t for t in self.results['connection_tests'] if t['success']]
        if connection_tests:
            metrics['connection'] = {
                'avg_duration_ms': statistics.mean([t['duration_ms'] for t in connection_tests]),
                'p95_duration_ms': statistics.quantiles([t['duration_ms'] for t in connection_tests], n=20)[18],
                'success_rate': len(connection_tests) / len(self.results['connection_tests'])
            }
        
        # Query metrics by type
        query_types = {}
        for test in self.results['query_tests']:
            test_name = test['test_name']
            if test_name not in query_types:
                query_types[test_name] = []
            query_types[test_name].append(test)
        
        metrics['queries'] = {}
        for test_name, tests in query_types.items():
            successful_tests = [t for t in tests if t['success']]
            if successful_tests:
                metrics['queries'][test_name] = {
                    'avg_duration_ms': statistics.mean([t['duration_ms'] for t in successful_tests]),
                    'p95_duration_ms': statistics.quantiles([t['duration_ms'] for t in successful_tests], n=20)[18],
                    'avg_rows': statistics.mean([t['row_count'] for t in successful_tests]),
                    'success_rate': len(successful_tests) / len(tests)
                }
        
        # Concurrent query metrics
        concurrent_tests = [t for t in self.results['concurrent_tests'] if t['success']]
        if concurrent_tests:
            metrics['concurrent_queries'] = {
                'avg_duration_ms': statistics.mean([t['duration_ms'] for t in concurrent_tests]),
                'p95_duration_ms': statistics.quantiles([t['duration_ms'] for t in concurrent_tests], n=20)[18],
                'total_rows': sum([t['row_count'] for t in concurrent_tests]),
                'success_rate': len(concurrent_tests) / len(self.results['concurrent_tests'])
            }
        
        # Throughput metrics
        if self.results['throughput_tests']:
            metrics['throughput'] = self.results['throughput_tests'][0]
        
        return metrics

async def main():
    # Database configuration
    db_config = {
        'host': 'localhost',
        'port': 5432,
        'user': 'testuser',
        'password': 'testpass',
        'database': 'silhouette_workflow_test'
    }
    
    tester = DatabaseLoadTester(db_config)
    
    try:
        # Initialize database connection
        await tester.initialize_pool()
        
        # Run load tests
        await tester.test_connection_performance()
        await tester.test_query_performance()
        await tester.test_concurrent_queries()
        await tester.test_throughput()
        await tester.test_error_handling()
        
        # Generate report
        await tester.generate_report('database-load-test-report.json')
        
    except Exception as e:
        print(f"âŒ Load test failed: {e}")
        raise
    
    finally:
        await tester.cleanup()

if __name__ == "__main__":
    asyncio.run(main())
```

### Database Connection Pool Testing

```python
#!/usr/bin/env python3
# load-tests/database/connection-pool-test.py
import asyncio
import asyncpg
import time
import statistics
from typing import List, Dict, Any

class ConnectionPoolLoadTester:
    def __init__(self, db_config: Dict[str, Any]):
        self.db_config = db_config
        self.pools = {}
        self.results = []
    
    async def create_pool(self, pool_name: str, min_size: int, max_size: int):
        """Create a connection pool with specified parameters"""
        try:
            pool = await asyncpg.create_pool(
                host=self.db_config['host'],
                port=self.db_config['port'],
                user=self.db_config['user'],
                password=self.db_config['password'],
                database=self.db_config['database'],
                min_size=min_size,
                max_size=max_size,
                command_timeout=60
            )
            
            self.pools[pool_name] = pool
            print(f"âœ… Created pool '{pool_name}': min_size={min_size}, max_size={max_size}")
            return pool
            
        except Exception as e:
            print(f"âŒ Failed to create pool '{pool_name}': {e}")
            raise
    
    async def test_pool_performance(self, pool_name: str, test_duration: int = 30):
        """Test connection pool performance under load"""
        print(f"ğŸ” Testing pool '{pool_name}' performance...")
        
        pool = self.pools[pool_name]
        start_time = time.time()
        operations_completed = 0
        errors = 0
        response_times = []
        
        async def perform_operation():
            nonlocal operations_completed, errors
            try:
                operation_start = time.time()
                
                async with pool.acquire() as conn:
                    # Perform various database operations
                    result = await conn.fetchval('SELECT COUNT(*) FROM users')
                    await conn.execute('SELECT 1')  # Additional query
                
                operation_end = time.time()
                duration = (operation_end - operation_start) * 1000
                response_times.append(duration)
                operations_completed += 1
                
            except Exception as e:
                errors += 1
                print(f"Operation error: {e}")
        
        # Continuous operation loop
        while time.time() - start_time < test_duration:
            # Create multiple concurrent operations
            concurrent_ops = min(pool.get_max_size(), 20)  # Don't exceed pool size
            
            tasks = [perform_operation() for _ in range(concurrent_ops)]
            await asyncio.gather(*tasks, return_exceptions=True)
            
            # Small delay to prevent overwhelming the database
            await asyncio.sleep(0.1)
        
        end_time = time.time()
        total_duration = end_time - start_time
        
        # Calculate metrics
        ops_per_second = operations_completed / total_duration
        error_rate = errors / (operations_completed + errors) if (operations_completed + errors) > 0 else 0
        
        avg_response_time = statistics.mean(response_times) if response_times else 0
        p95_response_time = statistics.quantiles(response_times, n=20)[18] if len(response_times) > 20 else 0
        
        result = {
            'pool_name': pool_name,
            'test_duration': total_duration,
            'operations_completed': operations_completed,
            'operations_per_second': ops_per_second,
            'errors': errors,
            'error_rate': error_rate,
            'avg_response_time_ms': avg_response_time,
            'p95_response_time_ms': p95_response_time,
            'pool_config': {
                'min_size': pool.get_min_size(),
                'max_size': pool.get_max_size()
            }
        }
        
        self.results.append(result)
        
        print(f"âœ… Pool '{pool_name}' test completed:")
        print(f"   Operations: {operations_completed}")
        print(f"   Throughput: {ops_per_second:.2f} ops/sec")
        print(f"   Error rate: {error_rate:.2%}")
        print(f"   Avg response time: {avg_response_time:.2f}ms")
        print(f"   95th percentile: {p95_response_time:.2f}ms")
    
    async def test_pool_exhaustion(self, pool_name: str):
        """Test behavior when connection pool is exhausted"""
        print(f"ğŸ” Testing pool '{pool_name}' exhaustion behavior...")
        
        pool = self.pools[pool_name]
        acquired_connections = []
        
        try:
            # Acquire all connections in the pool
            for i in range(pool.get_max_size()):
                conn = await pool.acquire()
                acquired_connections.append(conn)
                print(f"   Acquired connection {i+1}/{pool.get_max_size()}")
            
            # Try to acquire one more connection (should fail or timeout)
            try:
                extra_conn = await asyncio.wait_for(
                    pool.acquire(), 
                    timeout=5.0
                )
                print("   âš ï¸ Extra connection acquired (unexpected)")
                await pool.release(extra_conn)
                
            except asyncio.TimeoutError:
                print("   âœ… Pool exhaustion detected (timeout as expected)")
            
            # Test operations with full pool
            start_time = time.time()
            for i, conn in enumerate(acquired_connections):
                try:
                    await conn.fetchval('SELECT 1')
                except Exception as e:
                    print(f"   âŒ Connection {i} operation failed: {e}")
            end_time = time.time()
            
            total_duration = (end_time - start_time) * 1000
            avg_per_connection = total_duration / len(acquired_connections)
            
            print(f"   Operations with full pool: {total_duration:.2f}ms total, {avg_per_connection:.2f}ms avg per connection")
            
        finally:
            # Release all connections
            for conn in acquired_connections:
                await pool.release(conn)
    
    async def test_pool_recovery(self, pool_name: str):
        """Test pool recovery after connection failures"""
        print(f"ğŸ” Testing pool '{pool_name}' recovery behavior...")
        
        pool = self.pools[pool_name]
        
        # Simulate connection failures
        for i in range(5):
            try:
                async with pool.acquire() as conn:
                    # Force close the connection (simulate failure)
                    await conn.execute('SELECT pg_terminate_backend(pg_backend_pid())')
                    
            except Exception as e:
                print(f"   Connection {i+1} terminated: {type(e).__name__}")
        
        # Test that pool continues to work
        start_time = time.time()
        try:
            async with pool.acquire() as conn:
                result = await conn.fetchval('SELECT 1')
            end_time = time.time()
            
            duration = (end_time - start_time) * 1000
            print(f"   âœ… Pool recovery successful: {duration:.2f}ms")
            
        except Exception as e:
            print(f"   âŒ Pool recovery failed: {e}")
    
    async def cleanup(self):
        """Close all connection pools"""
        for pool_name, pool in self.pools.items():
            try:
                await pool.close()
                print(f"âœ… Closed pool '{pool_name}'")
            except Exception as e:
                print(f"âš ï¸ Error closing pool '{pool_name}': {e}")
    
    async def generate_report(self, output_file: str):
        """Generate connection pool test report"""
        import json
        
        report = {
            'test_timestamp': time.time(),
            'database_config': self.db_config,
            'pool_configs': {
                name: {
                    'min_size': pool.get_min_size(),
                    'max_size': pool.get_max_size()
                }
                for name, pool in self.pools.items()
            },
            'test_results': self.results
        }
        
        with open(output_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"âœ… Connection pool report saved to: {output_file}")

async def main():
    # Database configuration
    db_config = {
        'host': 'localhost',
        'port': 5432,
        'user': 'testuser',
        'password': 'testpass',
        'database': 'silhouette_workflow_test'
    }
    
    tester = ConnectionPoolLoadTester(db_config)
    
    try:
        # Create different pool configurations
        await tester.create_pool('small_pool', min_size=5, max_size=10)
        await tester.create_pool('medium_pool', min_size=10, max_size=25)
        await tester.create_pool('large_pool', min_size=25, max_size=50)
        
        # Test each pool
        for pool_name in tester.pools.keys():
            await tester.test_pool_performance(pool_name, test_duration=30)
            await tester.test_pool_exhaustion(pool_name)
            await tester.test_pool_recovery(pool_name)
        
        # Generate report
        await tester.generate_report('connection-pool-test-report.json')
        
    except Exception as e:
        print(f"âŒ Connection pool test failed: {e}")
        raise
    
    finally:
        await tester.cleanup()

if __name__ == "__main__":
    asyncio.run(main())
```

## 3. API Load Testing

### Comprehensive API Load Test

```javascript
// load-tests/api/api-load-test.js
import http from 'k6/http';
import { check, sleep, group } from 'k6';
import { Rate, Trend, Counter } from 'k6/metrics';

// Custom metrics
export const errorRate = new Rate('errors');
export const responseTime = new Trend('response_time');
export const apiThroughput = new Rate('api_throughput');
export const concurrentRequests = new Counter('concurrent_requests');

// API load test configuration
export const options = {
  scenarios: {
    // API endpoint specific load
    workflow_api_load: {
      executor: 'constant-vus',
      vus: 100,
      duration: '15m',
      startTime: '0s',
    },
    auth_api_load: {
      executor: 'constant-vus',
      vus: 50,
      duration: '15m',
      startTime: '0s',
    },
    analytics_api_load: {
      executor: 'constant-vus',
      vus: 30,
      duration: '15m',
      startTime: '0s',
    },
    search_api_load: {
      executor: 'constant-vus',
      vus: 25,
      duration: '15m',
      startTime: '0s',
    },
  },
  thresholds: {
    http_req_duration: ['p(95)<800', 'p(99)<1500'], // 95% under 800ms, 99% under 1.5s
    http_req_failed: ['rate<0.01'], // Error rate under 1%
    errors: ['rate<0.01'],
    api_throughput: ['rate>1000'], // Minimum 1000 requests per second
  },
};

const BASE_URL = __ENV.BASE_URL || 'http://localhost:8000/api';
const API_KEY = __ENV.API_KEY || 'test-api-key';

export function setup() {
  console.log(`Starting API load test`);
  console.log(`Base URL: ${BASE_URL}`);
  console.log(`API Key: ${API_KEY ? '***' + API_KEY.slice(-4) : 'not provided'}`);

  return { 
    baseUrl: BASE_URL,
    apiKey: API_KEY,
    endpoints: {
      // Authentication endpoints
      login: '/auth/login',
      logout: '/auth/logout',
      profile: '/auth/profile',
      refresh: '/auth/refresh',
      
      // Workflow endpoints
      workflows: '/workflows',
      workflowById: '/workflows/{id}',
      executeWorkflow: '/workflows/{id}/execute',
      workflowMetrics: '/workflows/{id}/metrics',
      workflowTemplates: '/workflows/templates',
      
      // User management endpoints
      users: '/users',
      userById: '/users/{id}',
      userWorkflows: '/users/{id}/workflows',
      
      // Analytics endpoints
      analytics: {
        dashboard: '/analytics/dashboard',
        realtime: '/analytics/realtime',
        reports: '/analytics/reports',
        metrics: '/analytics/metrics',
        trends: '/analytics/trends',
      },
      
      // Search endpoints
      search: '/search',
      searchWorkflows: '/search/workflows',
      searchUsers: '/search/users',
      
      // System endpoints
      health: '/health',
      metrics: '/metrics',
      status: '/status',
    }
  };
}

export default function (data) {
  const { baseUrl, apiKey, endpoints } = data;
  
  // Track concurrent requests
  concurrentRequests.add(1);
  
  // Group API tests by functionality
  group('Authentication API Tests', () => {
    testAuthenticationAPI(baseUrl, endpoints, apiKey);
  });
  
  group('Workflow API Tests', () => {
    testWorkflowAPI(baseUrl, endpoints, apiKey);
  });
  
  group('User Management API Tests', () => {
    testUserManagementAPI(baseUrl, endpoints, apiKey);
  });
  
  group('Analytics API Tests', () => {
    testAnalyticsAPI(baseUrl, endpoints, apiKey);
  });
  
  group('Search API Tests', () => {
    testSearchAPI(baseUrl, endpoints, apiKey);
  });
  
  group('System API Tests', () => {
    testSystemAPI(baseUrl, endpoints, apiKey);
  });
  
  // Simulate realistic API usage patterns
  sleep(Math.random() * 2 + 1); // 1-3 seconds between API calls
}

function testAuthenticationAPI(baseUrl, endpoints, apiKey) {
  const headers = {
    'Content-Type': 'application/json',
    'X-API-Key': apiKey,
  };
  
  // Test user profile endpoint (lightweight operation)
  const profileResponse = http.get(
    `${baseUrl}${endpoints.profile}`,
    { headers }
  );
  
  const profileCheck = check(profileResponse, {
    'profile API status 200': (r) => r.status === 200,
    'profile API response time < 200ms': (r) => r.timings.duration < 200,
    'profile API has user data': (r) => {
      const body = r.json('data');
      return body && body.id && body.email;
    },
  });
  
  errorRate.add(!profileCheck);
  responseTime.add(profileResponse.timings.duration);
  apiThroughput.add(1);
  
  // Test token refresh occasionally
  if (Math.random() < 0.1) {
    const refreshResponse = http.post(
      `${baseUrl}${endpoints.refresh}`,
      {},
      { headers }
    );
    
    const refreshCheck = check(refreshResponse, {
      'refresh API status 200': (r) => r.status === 200,
      'refresh API response time < 300ms': (r) => r.timings.duration < 300,
    });
    
    errorRate.add(!refreshCheck);
    responseTime.add(refreshResponse.timings.duration);
    apiThroughput.add(1);
  }
}

function testWorkflowAPI(baseUrl, endpoints, apiKey) {
  const headers = {
    'Content-Type': 'application/json',
    'X-API-Key': apiKey,
  };
  
  // Test workflow listing (most common operation)
  const listResponse = http.get(
    `${baseUrl}${endpoints.workflows}?page=1&limit=20&sort=createdAt&order=desc`,
    { headers }
  );
  
  const listCheck = check(listResponse, {
    'workflow list API status 200': (r) => r.status === 200,
    'workflow list API response time < 500ms': (r) => r.timings.duration < 500,
    'workflow list API has pagination': (r) => {
      const body = r.json('pagination');
      return body && body.page && body.total;
    },
  });
  
  errorRate.add(!listCheck);
  responseTime.add(listResponse.timings.duration);
  apiThroughput.add(1);
  
  // Test workflow creation occasionally
  if (Math.random() < 0.15) {
    const workflowData = {
      name: `API Load Test Workflow ${Date.now()}`,
      description: 'Load testing workflow via API',
      triggers: [{ type: 'manual' }],
      actions: [
        {
          type: 'api_call',
          config: {
            url: 'https://httpbin.org/get',
            method: 'GET',
            timeout: 10000,
          },
        },
      ],
    };
    
    const createResponse = http.post(
      `${baseUrl}${endpoints.workflows}`,
      JSON.stringify(workflowData),
      { headers }
    );
    
    const createCheck = check(createResponse, {
      'workflow create API status 201': (r) => r.status === 201,
      'workflow create API response time < 800ms': (r) => r.timings.duration < 800,
      'workflow create API has workflow data': (r) => {
        const body = r.json('data');
        return body && body.id && body.name;
      },
    });
    
    errorRate.add(!createCheck);
    responseTime.add(createResponse.timings.duration);
    apiThroughput.add(1);
  }
  
  // Test workflow execution
  if (Math.random() < 0.2) {
    // Get a workflow ID to execute
    const workflowId = Math.floor(Math.random() * 1000) + 1; // Simulated ID
    
    const executeResponse = http.post(
      `${baseUrl}${endpoints.executeWorkflow.replace('{id}', workflowId)}`,
      JSON.stringify({ trigger: 'api' }),
      { headers }
    );
    
    const executeCheck = check(executeResponse, {
      'workflow execute API status 200': (r) => r.status === 200,
      'workflow execute API response time < 1000ms': (r) => r.timings.duration < 1000,
      'workflow execute API started': (r) => {
        const body = r.json('data');
        return body && body.status === 'running';
      },
    });
    
    errorRate.add(!executeCheck);
    responseTime.add(executeResponse.timings.duration);
    apiThroughput.add(1);
  }
}

function testUserManagementAPI(baseUrl, endpoints, apiKey) {
  const headers = {
    'Content-Type': 'application/json',
    'X-API-Key': apiKey,
  };
  
  // Test user profile via different endpoint
  const userResponse = http.get(
    `${baseUrl}/users/me`,
    { headers }
  );
  
  const userCheck = check(userResponse, {
    'user API status 200': (r) => r.status === 200,
    'user API response time < 300ms': (r) => r.timings.duration < 300,
    'user API has user data': (r) => {
      const body = r.json('data');
      return body && body.id && body.email;
    },
  });
  
  errorRate.add(!userCheck);
  responseTime.add(userResponse.timings.duration);
  apiThroughput.add(1);
  
  // Test user activity occasionally
  if (Math.random() < 0.05) {
    const activityResponse = http.get(
      `${baseUrl}/users/me/activity`,
      { headers }
    );
    
    const activityCheck = check(activityResponse, {
      'user activity API status 200': (r) => r.status === 200,
      'user activity API response time < 500ms': (r) => r.timings.duration < 500,
    });
    
    errorRate.add(!activityCheck);
    responseTime.add(activityResponse.timings.duration);
    apiThroughput.add(1);
  }
}

function testAnalyticsAPI(baseUrl, endpoints, apiKey) {
  const headers = {
    'Content-Type': 'application/json',
    'X-API-Key': apiKey,
  };
  
  // Test analytics dashboard
  const dashboardResponse = http.get(
    `${baseUrl}${endpoints.analytics.dashboard}`,
    { headers }
  );
  
  const dashboardCheck = check(dashboardResponse, {
    'analytics dashboard API status 200': (r) => r.status === 200,
    'analytics dashboard API response time < 800ms': (r) => r.timings.duration < 800,
    'analytics dashboard has metrics': (r) => {
      const body = r.json('data');
      return body && body.summary;
    },
  });
  
  errorRate.add(!dashboardCheck);
  responseTime.add(dashboardResponse.timings.duration);
  apiThroughput.add(1);
  
  // Test real-time analytics
  const realtimeResponse = http.get(
    `${baseUrl}${endpoints.analytics.realtime}`,
    { headers }
  );
  
  const realtimeCheck = check(realtimeResponse, {
    'analytics realtime API status 200': (r) => r.status === 200,
    'analytics realtime API response time < 300ms': (r) => r.timings.duration < 300,
  });
  
  errorRate.add(!realtimeCheck);
  responseTime.add(realtimeResponse.timings.duration);
  apiThroughput.add(1);
  
  // Test metrics occasionally
  if (Math.random() < 0.3) {
    const metricsResponse = http.get(
      `${baseUrl}${endpoints.analytics.metrics}?timeframe=24h`,
      { headers }
    );
    
    const metricsCheck = check(metricsResponse, {
      'analytics metrics API status 200': (r) => r.status === 200,
      'analytics metrics API response time < 600ms': (r) => r.timings.duration < 600,
    });
    
    errorRate.add(!metricsCheck);
    responseTime.add(metricsResponse.timings.duration);
    apiThroughput.add(1);
  }
}

function testSearchAPI(baseUrl, endpoints, apiKey) {
  const headers = {
    'Content-Type': 'application/json',
    'X-API-Key': apiKey,
  };
  
  // Test search with various queries
  const searchQueries = [
    'workflow',
    'user',
    'analytics',
    'dashboard',
    'automation',
  ];
  
  const randomQuery = searchQueries[Math.floor(Math.random() * searchQueries.length)];
  
  const searchResponse = http.get(
    `${baseUrl}${endpoints.search}?q=${encodeURIComponent(randomQuery)}&limit=10`,
    { headers }
  );
  
  const searchCheck = check(searchResponse, {
    'search API status 200': (r) => r.status === 200,
    'search API response time < 600ms': (r) => r.timings.duration < 600,
    'search API has results': (r) => {
      const body = r.json('data');
      return Array.isArray(body);
    },
  });
  
  errorRate.add(!searchCheck);
  responseTime.add(searchResponse.timings.duration);
  apiThroughput.add(1);
  
  // Test specific search endpoints
  if (Math.random() < 0.4) {
    const workflowSearchResponse = http.get(
      `${baseUrl}${endpoints.searchWorkflows}?q=${randomQuery}&status=active&limit=5`,
      { headers }
    );
    
    const workflowSearchCheck = check(workflowSearchResponse, {
      'workflow search API status 200': (r) => r.status === 200,
      'workflow search API response time < 400ms': (r) => r.timings.duration < 400,
    });
    
    errorRate.add(!workflowSearchCheck);
    responseTime.add(workflowSearchResponse.timings.duration);
    apiThroughput.add(1);
  }
}

function testSystemAPI(baseUrl, endpoints, apiKey) {
  const headers = {
    'X-API-Key': apiKey,
  };
  
  // Test health endpoint
  const healthResponse = http.get(
    `${baseUrl}${endpoints.health}`,
    { headers }
  );
  
  const healthCheck = check(healthResponse, {
    'health API status 200': (r) => r.status === 200,
    'health API response time < 100ms': (r) => r.timings.duration < 100,
    'health API is healthy': (r) => {
      const body = r.json();
      return body.status === 'healthy';
    },
  });
  
  errorRate.add(!healthCheck);
  responseTime.add(healthResponse.timings.duration);
  apiThroughput.add(1);
  
  // Test status endpoint
  const statusResponse = http.get(
    `${baseUrl}${endpoints.status}`,
    { headers }
  );
  
  const statusCheck = check(statusResponse, {
    'status API status 200': (r) => r.status === 200,
    'status API response time < 150ms': (r) => r.timings.duration < 150,
  });
  
  errorRate.add(!statusCheck);
  responseTime.add(statusResponse.timings.duration);
  apiThroughput.add(1);
  
  // Test metrics endpoint (if accessible)
  if (Math.random() < 0.1) {
    const metricsResponse = http.get(
      `${baseUrl}${endpoints.metrics}`,
      { headers }
    );
    
    const metricsCheck = check(metricsResponse, {
      'metrics API status 200': (r) => r.status === 200,
      'metrics API response time < 200ms': (r) => r.timings.duration < 200,
    });
    
    errorRate.add(!metricsCheck);
    responseTime.add(metricsResponse.timings.duration);
    apiThroughput.add(1);
  }
}
```

### Rate Limiting Test

```javascript
// load-tests/api/rate-limiting-test.js
import http from 'k6/http';
import { check, sleep } from 'k6';
import { Rate, Trend, Counter } from 'k6/metrics';

// Custom metrics
export const rateLimitHits = new Rate('rate_limit_hits');
export const rateLimitedRequests = new Rate('rate_limited_requests');
export const successfulRequests = new Rate('successful_requests');

export const options = {
  scenarios: {
    rate_limit_test: {
      executor: 'constant-vus',
      vus: 10,  # Small number to clearly see rate limiting
      duration: '5m',
    },
  },
  thresholds: {
    rate_limit_hits: ['rate>0.1'],  # At least 10% should hit rate limits
    rate_limited_requests: ['rate<0.5'],  # But not more than 50%
  },
};

const BASE_URL = __ENV.BASE_URL || 'http://localhost:8000/api';
const RATE_LIMIT_ENDPOINT = '/workflows';  # Endpoint to test rate limiting on

export default function (data) {
  const headers = {
    'Content-Type': 'application/json',
    'Authorization': 'Bearer test-token',
  };
  
  // Make rapid requests to trigger rate limiting
  for (let i = 0; i < 5; i++) {
    const response = http.get(`${BASE_URL}${RATE_LIMIT_ENDPOINT}`, { headers });
    
    if (response.status === 429) {
      // Rate limited
      rateLimitHits.add(1);
      rateLimitedRequests.add(1);
      
      check(response, {
        'rate limit response contains limit info': (r) => {
          const body = r.json();
          return body.error && body.error.includes('rate limit');
        },
      });
      
      // Extract retry-after header if present
      const retryAfter = response.headers['Retry-After'];
      if (retryAfter) {
        const waitTime = parseInt(retryAfter) * 1000;
        sleep(waitTime / 1000);  # Wait for the rate limit to reset
      } else {
        sleep(1);  # Default wait
      }
      
    } else if (response.status === 200) {
      // Successful request
      successfulRequests.add(1);
      
      check(response, {
        'successful response is valid': (r) => r.status === 200 && r.json('success') === true,
      });
      
      // Small delay to avoid overwhelming
      sleep(0.1);
      
    } else {
      // Other error
      check(response, {
        'unexpected status code': (r) => r.status < 500,  # Should not be server error
      });
    }
  }
}
```

## 4. Memory Leak Detection

### Node.js Memory Leak Testing

```javascript
// load-tests/memory/memory-leak-test.js
import http from 'k6/http';
import { check } from 'k6';

export const options = {
  scenarios: {
    memory_leak_test: {
      executor: 'constant-vus',
      vus: 50,
      duration: '30m',  # 30 minutes to detect memory leaks
    },
  },
};

const BASE_URL = __ENV.BASE_URL || 'http://localhost:8000/api';

export default function () {
  // Test for memory leaks in long-running operations
  
  // 1. Test workflow execution memory usage
  testWorkflowExecutionMemory();
  
  // 2. Test large data processing memory usage
  testLargeDataProcessingMemory();
  
  // 3. Test database connection memory usage
  testDatabaseConnectionMemory();
  
  // 4. Test WebSocket connection memory usage
  testWebSocketMemory();
  
  // 5. Test garbage collection behavior
  testGarbageCollectionBehavior();
}

function testWorkflowExecutionMemory() {
  const headers = {
    'Content-Type': 'application/json',
    'Authorization': 'Bearer test-token',
  };
  
  // Create a workflow with memory-intensive operations
  const workflowData = {
    name: `Memory Test Workflow ${Date.now()}`,
    description: 'Testing memory usage during execution',
    triggers: [{ type: 'manual' }],
    actions: [
      {
        type: 'data_processing',
        config: {
          operation: 'process_large_dataset',
          size: 10000,  # Process 10,000 items
          memory_intensive: true,
        },
      },
      {
        type: 'api_call',
        config: {
          url: 'https://httpbin.org/post',
          method: 'POST',
          body: generateLargePayload(50000),  # 50KB payload
        },
      },
    ],
  };
  
  const createResponse = http.post(
    `${BASE_URL}/workflows`,
    JSON.stringify(workflowData),
    { headers }
  );
  
  check(createResponse, {
    'workflow creation successful': (r) => r.status === 201,
  });
  
  if (createResponse.status === 201) {
    const workflowId = createResponse.json('data.id');
    
    // Execute workflow multiple times
    for (let i = 0; i < 10; i++) {
      const executeResponse = http.post(
        `${BASE_URL}/workflows/${workflowId}/execute`,
        JSON.stringify({ trigger: 'manual' }),
        { headers }
      );
      
      check(executeResponse, {
        'workflow execution started': (r) => r.status === 200,
      });
      
      // Wait a bit between executions
      sleep(1);
    }
  }
}

function testLargeDataProcessingMemory() {
  const headers = {
    'Content-Type': 'application/json',
    'Authorization': 'Bearer test-token',
  };
  
  // Test endpoints that process large amounts of data
  const largeDataEndpoints = [
    '/analytics/reports?size=large',
    '/search?q=test&limit=1000',
    '/workflows?limit=500&include=executions',
  ];
  
  const randomEndpoint = largeDataEndpoints[Math.floor(Math.random() * largeDataEndpoints.length)];
  
  const response = http.get(`${BASE_URL}${randomEndpoint}`, { headers });
  
  check(response, {
    'large data endpoint responds': (r) => r.status === 200,
    'large data response time reasonable': (r) => r.timings.duration < 5000,  # 5 seconds
  });
}

function testDatabaseConnectionMemory() {
  const headers = {
    'Content-Type': 'application/json',
    'Authorization': 'Bearer test-token',
  };
  
  // Test database-heavy operations
  const dbEndpoints = [
    '/workflows?sort=createdAt&order=desc&limit=100',
    '/analytics/dashboard',
    '/users/me/activity?limit=200',
  ];
  
  const randomEndpoint = dbEndpoints[Math.floor(Math.random() * dbEndpoints.length)];
  
  const response = http.get(`${BASE_URL}${randomEndpoint}`, { headers });
  
  check(response, {
    'database endpoint responds': (r) => r.status === 200,
  });
}

function testWebSocketMemory() {
  // This would test WebSocket connections for memory leaks
  // In a real implementation, you would use a WebSocket library for k6
  
  // For now, simulate WebSocket-like behavior with repeated requests
  const headers = {
    'Content-Type': 'application/json',
    'Authorization': 'Bearer test-token',
  };
  
  // Test real-time endpoints that might use WebSockets
  const realtimeEndpoints = [
    '/analytics/realtime',
    '/workflows/stream',
    '/notifications/stream',
  ];
  
  const randomEndpoint = realtimeEndpoints[Math.floor(Math.random() * realtimeEndpoints.length)];
  
  const response = http.get(`${BASE_URL}${randomEndpoint}`, { headers });
  
  check(response, {
    'realtime endpoint responds': (r) => r.status === 200,
  });
}

function testGarbageCollectionBehavior() {
  const headers = {
    'Content-Type': 'application/json',
    'Authorization': 'Bearer test-token',
  };
  
  // Test endpoints that create many objects
  const memoryTestEndpoints = [
    '/workflows?create_temp_data=true&count=100',
    '/analytics/stress_test?operations=50',
    '/users/bulk_create?count=20',
  ];
  
  const randomEndpoint = memoryTestEndpoints[Math.floor(Math.random() * memoryTestEndpoints.length)];
  
  const response = http.get(`${BASE_URL}${randomEndpoint}`, { headers });
  
  check(response, {
    'memory test endpoint responds': (r) => r.status === 200,
  });
}

function generateLargePayload(size) {
  // Generate a large payload for testing
  const chunk = 'A'.repeat(1000);
  const chunks = Math.floor(size / 1000);
  return Array(chunks).fill(chunk).join('');
}
```

### Memory Monitoring Script

```python
#!/usr/bin/env python3
# load-tests/memory/memory-monitor.py
import psutil
import time
import json
import threading
from datetime import datetime
import statistics

class MemoryMonitor:
    def __init__(self, process_name="node", interval=1.0):
        self.process_name = process_name
        self.interval = interval
        self.monitoring = False
        self.measurements = []
        self.alerts = []
    
    def find_node_processes(self):
        """Find Node.js processes"""
        processes = []
        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
            try:
                if self.process_name.lower() in proc.info['name'].lower():
                    processes.append(proc)
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                continue
        return processes
    
    def monitor_process(self, proc):
        """Monitor a specific process for memory usage"""
        try:
            # Get memory information
            memory_info = proc.memory_info()
            memory_percent = proc.memory_percent()
            
            # Get memory statistics
            memory_stats = proc.memory_stats()
            
            return {
                'timestamp': datetime.now().isoformat(),
                'pid': proc.pid,
                'name': proc.name(),
                'memory_rss_mb': memory_info.rss / 1024 / 1024,  # Resident Set Size in MB
                'memory_vms_mb': memory_info.vms / 1024 / 1024,  # Virtual Memory Size in MB
                'memory_percent': memory_percent,
                'memory_stats': {
                    'rss': memory_stats.rss,
                    'vms': memory_stats.vms,
                    'page_faults': memory_stats.page_faults,
                    'page_faults_minor': memory_stats.page_faults_minor,
                },
                'num_threads': proc.num_threads(),
                'cpu_percent': proc.cpu_percent(),
            }
        except (psutil.NoSuchProcess, psutil.AccessDenied) as e:
            return {
                'timestamp': datetime.now().isoformat(),
                'pid': proc.pid,
                'error': str(e),
            }
    
    def start_monitoring(self):
        """Start memory monitoring"""
        self.monitoring = True
        self.measurements = []
        
        def monitor_loop():
            while self.monitoring:
                processes = self.find_node_processes()
                
                for proc in processes:
                    measurement = self.monitor_process(proc)
                    self.measurements.append(measurement)
                    
                    # Check for memory leak indicators
                    self.check_memory_leak(measurement)
                
                time.sleep(self.interval)
        
        self.monitor_thread = threading.Thread(target=monitor_loop, daemon=True)
        self.monitor_thread.start()
        print(f"ğŸ” Started memory monitoring (interval: {self.interval}s)")
    
    def stop_monitoring(self):
        """Stop memory monitoring"""
        self.monitoring = False
        if hasattr(self, 'monitor_thread'):
            self.monitor_thread.join(timeout=5)
        print("ğŸ›‘ Stopped memory monitoring")
    
    def check_memory_leak(self, measurement):
        """Check for memory leak indicators"""
        if 'error' in measurement:
            return
        
        current_rss = measurement['memory_rss_mb']
        current_time = datetime.fromisoformat(measurement['timestamp'])
        
        # Check for continuous memory growth
        if len(self.measurements) > 10:
            recent_measurements = [m for m in self.measurements[-10:] if 'error' not in m]
            if len(recent_measurements) > 5:
                rss_values = [m['memory_rss_mb'] for m in recent_measurements]
                if len(rss_values) >= 5:
                    # Check if memory is consistently increasing
                    increasing = all(rss_values[i] <= rss_values[i+1] for i in range(len(rss_values)-2))
                    growth_rate = (rss_values[-1] - rss_values[0]) / len(rss_values)
                    
                    if increasing and growth_rate > 10:  # More than 10MB growth per measurement
                        self.alerts.append({
                            'timestamp': current_time.isoformat(),
                            'type': 'memory_leak_suspected',
                            'pid': measurement['pid'],
                            'current_memory_mb': current_rss,
                            'growth_rate_mb_per_measurement': growth_rate,
                            'message': f'Suspected memory leak: {growth_rate:.2f}MB growth per measurement'
                        })
        
        # Check for high memory usage
        if current_rss > 1000:  # More than 1GB
            self.alerts.append({
                'timestamp': current_time.isoformat(),
                'type': 'high_memory_usage',
                'pid': measurement['pid'],
                'current_memory_mb': current_rss,
                'message': f'High memory usage detected: {current_rss:.2f}MB'
            })
        
        # Check for rapid memory increase
        if len(self.measurements) > 1:
            prev_measurement = None
            for m in reversed(self.measurements):
                if 'error' not in m and m['pid'] == measurement['pid']:
                    prev_measurement = m
                    break
            
            if prev_measurement:
                memory_increase = current_rss - prev_measurement['memory_rss_mb']
                if memory_increase > 100:  # More than 100MB increase
                    self.alerts.append({
                        'timestamp': current_time.isoformat(),
                        'type': 'rapid_memory_increase',
                        'pid': measurement['pid'],
                        'memory_increase_mb': memory_increase,
                        'message': f'Rapid memory increase: {memory_increase:.2f}MB'
                    })
    
    def generate_report(self, output_file):
        """Generate memory monitoring report"""
        if not self.measurements:
            print("âš ï¸ No measurements collected")
            return
        
        # Calculate statistics
        valid_measurements = [m for m in self.measurements if 'error' not in m]
        
        if valid_measurements:
            rss_values = [m['memory_rss_mb'] for m in valid_measurements]
            vms_values = [m['memory_vms_mb'] for m in valid_measurements]
            
            report = {
                'monitoring_info': {
                    'start_time': valid_measurements[0]['timestamp'],
                    'end_time': valid_measurements[-1]['timestamp'],
                    'total_measurements': len(self.measurements),
                    'valid_measurements': len(valid_measurements),
                    'monitoring_interval_seconds': self.interval,
                },
                'memory_statistics': {
                    'rss_memory_mb': {
                        'min': min(rss_values),
                        'max': max(rss_values),
                        'avg': statistics.mean(rss_values),
                        'median': statistics.median(rss_values),
                        'stdev': statistics.stdev(rss_values) if len(rss_values) > 1 else 0,
                        'p95': statistics.quantiles(rss_values, n=20)[18] if len(rss_values) > 20 else max(rss_values),
                    },
                    'vms_memory_mb': {
                        'min': min(vms_values),
                        'max': max(vms_values),
                        'avg': statistics.mean(vms_values),
                        'median': statistics.median(vms_values),
                        'stdev': statistics.stdev(vms_values) if len(vms_values) > 1 else 0,
                    },
                },
                'alerts': self.alerts,
                'detailed_measurements': self.measurements,
                'analysis': self.analyze_memory_patterns(valid_measurements),
            }
        else:
            report = {
                'monitoring_info': {
                    'error': 'No valid measurements collected',
                },
                'alerts': self.alerts,
            }
        
        with open(output_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"âœ… Memory monitoring report saved to: {output_file}")
    
    def analyze_memory_patterns(self, measurements):
        """Analyze memory usage patterns"""
        if len(measurements) < 5:
            return {'message': 'Insufficient data for pattern analysis'}
        
        rss_values = [m['memory_rss_mb'] for m in measurements]
        
        # Calculate trend
        time_points = list(range(len(rss_values)))
        
        # Simple linear trend calculation
        n = len(time_points)
        sum_x = sum(time_points)
        sum_y = sum(rss_values)
        sum_xy = sum(x * y for x, y in zip(time_points, rss_values))
        sum_x2 = sum(x * x for x in time_points)
        
        if n * sum_x2 - sum_x * sum_x != 0:
            slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x * sum_x)
            trend = 'increasing' if slope > 0.1 else 'decreasing' if slope < -0.1 else 'stable'
        else:
            slope = 0
            trend = 'stable'
        
        # Check for memory leaks
        memory_leak_detected = trend == 'increasing' and slope > 5  # 5MB increase per measurement
        
        return {
            'trend': trend,
            'slope_mb_per_measurement': slope,
            'memory_leak_detected': memory_leak_detected,
            'total_memory_growth_mb': rss_values[-1] - rss_values[0] if len(rss_values) > 1 else 0,
            'peak_memory_mb': max(rss_values),
            'memory_stability': 'stable' if statistics.stdev(rss_values) < 50 else 'volatile',
        }

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='Memory leak monitoring for Node.js processes')
    parser.add_argument('--process', default='node', help='Process name to monitor')
    parser.add_argument('--interval', type=float, default=1.0, help='Monitoring interval in seconds')
    parser.add_argument('--duration', type=int, default=300, help='Monitoring duration in seconds')
    parser.add_argument('--output', default='memory-monitoring-report.json', help='Output file for report')
    
    args = parser.parse_args()
    
    monitor = MemoryMonitor(process_name=args.process, interval=args.interval)
    
    try:
        print(f"ğŸ” Starting memory monitoring for process: {args.process}")
        print(f"   Duration: {args.duration} seconds")
        print(f"   Interval: {args.interval} seconds")
        
        # Start monitoring
        monitor.start_monitoring()
        
        # Monitor for specified duration
        time.sleep(args.duration)
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ Monitoring interrupted by user")
    
    finally:
        # Stop monitoring and generate report
        monitor.stop_monitoring()
        monitor.generate_report(args.output)
        
        # Print summary
        if monitor.alerts:
            print(f"\nâš ï¸ {len(monitor.alerts)} alerts generated:")
            for alert in monitor.alerts:
                print(f"   {alert['type']}: {alert['message']}")
        else:
            print("\nâœ… No memory issues detected")

if __name__ == "__main__":
    main()
```

## ConclusiÃ³n

El **Load Testing Framework** de Silhouette Workflow Creation proporciona un sistema integral de testing de carga que garantiza que la plataforma pueda manejar cargas de trabajo altas de manera eficiente y confiable. Con testing de usuarios concurrentes, base de datos, API, detecciÃ³n de memory leaks y escalabilidad, este framework establece un estÃ¡ndar de performance de clase empresarial.

### Beneficios Principales

1. **Cobertura Completa**: Usuarios concurrentes, DB, API, memoria y escalabilidad
2. **Herramientas MÃºltiples**: K6, JMeter, Python para diferentes tipos de testing
3. **Real-world Simulation**: Comportamiento real de usuarios con think time
4. **DetecciÃ³n Proactiva**: Memory leaks, deadlocks, performance degradation
5. **Monitoreo Continuo**: MÃ©tricas en tiempo real durante el testing
6. **Reportes AutomÃ¡ticos**: AnÃ¡lisis detallado de performance y recomendaciones
7. **Escalabilidad Validada**: Testing automÃ¡tico de auto-scaling y load balancing

### MÃ©tricas de Ã‰xito

- **Concurrent Users**: 1000+ usuarios simultÃ¡neos soportados
- **Database Performance**: P95 query time <100ms bajo carga
- **API Throughput**: 1000+ requests/second
- **Memory Usage**: Sin memory leaks en 30+ minutos de testing
- **Error Rate**: <1% bajo carga normal
- **Response Time**: P95 <800ms, P99 <1500ms
- **Resource Utilization**: CPU <80%, Memory <85%

---

**Autor**: Silhouette Anonimo  
**VersiÃ³n**: 1.0.0  
**Fecha**: 2025-11-09  
**Estado**: ImplementaciÃ³n Completa